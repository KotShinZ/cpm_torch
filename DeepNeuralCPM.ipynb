{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm  # プログレスバー表示用\n",
    "\n",
    "# <<< torchvision をインポート >>>\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# --- 設定クラス ---\n",
    "class Config:\n",
    "    # 使用デバイス (GPUが利用可能ならGPU、そうでなければCPU)\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- 格子関連 ---\n",
    "    # <<< MNISTに合わせる場合は28x28に近づけるか、リサイズを前提とする >>>\n",
    "    LATTICE_SIZE = 32  # MNIST(28x28)をリサイズして使う想定\n",
    "    NUM_CELL_TYPES = 3  # 0: 培地, 1: 背景細胞(タイプ1), 2: 数字細胞(タイプ2)\n",
    "    # <<< MNIST数字形成用のタイプID >>>\n",
    "    MNIST_TARGET_CELL_TYPE = 2\n",
    "\n",
    "    # --- 細胞関連 ---\n",
    "    # Neural Hamiltonianでのパディングやボリューム計算のため、想定される細胞の最大数を定義\n",
    "    # <<< MNISTデータを使う場合、厳密な細胞数は可変になる。ここでは最大値として定義 >>>\n",
    "    MAX_CELLS = 15  # 細胞の最大数 + 1 (培地用ID=0を含む) - これはNHの都合上の設定\n",
    "    TARGET_VOLUME = (\n",
    "        25  # 細胞の目標体積 (V*) - MNISTターゲットとは直接連動しない可能性あり\n",
    "    )\n",
    "    LAMBDA_VOLUME = 2.0  # 体積制約項の強度 (λ_v)\n",
    "\n",
    "    # --- 接触エネルギー J(type1, type2) ---\n",
    "    # タイプ1(背景)、タイプ2(数字)がそれぞれ凝集し、互いに反発するような設定例\n",
    "    J = torch.tensor(\n",
    "        [\n",
    "            [0, 4, 4],  # 培地 - 培地, 培地 - 背景, 培地 - 数字\n",
    "            [4, 2, 8],  # 背景 - 培地, 背景 - 背景, 背景 - 数字 (背景と数字は反発)\n",
    "            [4, 8, 2],  # 数字 - 培地, 数字 - 背景, 数字 - 数字\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    # --- Neural Hamiltonian (NH) アーキテクチャ関連 ---\n",
    "    NH_EMBED_DIM = 16\n",
    "    NH_HIDDEN_DIMS = [16, 32]\n",
    "    NH_KERNEL_SIZE = 3\n",
    "    NH_POOL_RATES = [2, 1]\n",
    "    NH_MLP_DIM = 64\n",
    "\n",
    "    # --- 学習関連 ---\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-4\n",
    "    TRAINING_STEPS = 5000\n",
    "    MCMC_STEPS_PER_UPDATE = 50\n",
    "    MCMC_PARALLEL_FLIPS = 100\n",
    "    MCMC_TEMP = 1.0\n",
    "    REGULARIZATION_LAMBDA = 0.01\n",
    "    PERSISTENT_CHAIN_PROB_RESET = 0.05\n",
    "    CLOSURE_WEIGHT_ANALYTICAL = 1.0  # 解析的ハミルトニアンの寄与を調整\n",
    "    CLOSURE_WEIGHT_NEURAL = (\n",
    "        5.0  # ニューラルハミルトニアンの寄与を強める (形状学習のため)\n",
    "    )\n",
    "\n",
    "    # --- MNISTデータ関連 ---\n",
    "    MNIST_DATA_PATH = \"./mnist_data\"  # MNISTデータの保存先\n",
    "    MNIST_THRESHOLD = 0.5  # MNIST画像を二値化する際の閾値\n",
    "\n",
    "    # --- 可視化関連 ---\n",
    "    VISUALIZE_EVERY = 100\n",
    "    NUM_VIZ_SAMPLES = 4\n",
    "\n",
    "\n",
    "# 設定クラスのインスタンスを作成\n",
    "cfg = Config()\n",
    "\n",
    "# --- MNISTデータセットの準備 ---\n",
    "# 画像の前処理: 指定サイズへのリサイズ、テンソル変換、(閾値処理は後で)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((cfg.LATTICE_SIZE, cfg.LATTICE_SIZE)),\n",
    "        transforms.ToTensor(),  # 画像を [0, 1] の範囲のテンソルに変換\n",
    "    ]\n",
    ")\n",
    "\n",
    "# MNIST訓練データセットをロード（なければダウンロード）\n",
    "try:\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root=cfg.MNIST_DATA_PATH, train=True, download=True, transform=transform\n",
    "    )\n",
    "    # データローダーを作成 (バッチ処理とシャッフルを行う)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "    # DataLoaderをイテレータに変換して、繰り返し使えるようにする\n",
    "    train_iter = iter(trainloader)\n",
    "    print(f\"MNIST dataset loaded/downloaded successfully from {cfg.MNIST_DATA_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading MNIST dataset: {e}\")\n",
    "    print(\n",
    "        \"Proceeding without MNIST data. `get_training_batch` will generate random data.\"\n",
    "    )\n",
    "    train_iter = None  # MNISTが使えない場合は None に設定\n",
    "\n",
    "\n",
    "# --- ヘルパー関数 (変更なし) ---\n",
    "def get_neighbors(lattice):\n",
    "    padded_lattice = F.pad(lattice.float(), (1, 1, 1, 1), mode=\"circular\").unsqueeze(1)\n",
    "    neighbors = []\n",
    "    for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "        neighbors.append(torch.roll(lattice, shifts=(dx, dy), dims=(-2, -1)))\n",
    "    neighbor_ids = torch.stack(neighbors, dim=1)\n",
    "    return neighbor_ids.long()\n",
    "\n",
    "\n",
    "def get_boundary_sites(lattice, neighbor_ids):\n",
    "    batch_size, H, W = lattice.shape\n",
    "    lattice_expanded = lattice.unsqueeze(1).expand(-1, 4, -1, -1)\n",
    "    is_different = lattice_expanded != neighbor_ids\n",
    "    boundary_mask = torch.any(is_different, dim=1)\n",
    "    boundary_mask = boundary_mask & (lattice != 0)\n",
    "    boundary_indices = [torch.nonzero(mask, as_tuple=False) for mask in boundary_mask]\n",
    "    return boundary_mask, boundary_indices\n",
    "\n",
    "\n",
    "def get_cell_volumes(lattice, max_cells):\n",
    "    one_hot = (\n",
    "        F.one_hot(lattice.long(), num_classes=max_cells).permute(0, 3, 1, 2).float()\n",
    "    )\n",
    "    volumes = one_hot.sum(dim=[2, 3])\n",
    "    return volumes\n",
    "\n",
    "\n",
    "# --- 解析的ハミルトニアン (Analytical Hamiltonian) ---\n",
    "# <<< MNISTターゲットの場合、細胞IDとタイプのマッピングが重要になる >>>\n",
    "class AnalyticalHamiltonian(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        self.J = config.J.to(config.DEVICE)\n",
    "        self.target_volume = config.TARGET_VOLUME\n",
    "        self.lambda_volume = config.LAMBDA_VOLUME\n",
    "        self.max_cells = config.MAX_CELLS\n",
    "\n",
    "    # <<< このマッピングは、MNISTターゲット生成方法と密接に関連 >>>\n",
    "    def map_ids_to_types(self, lattice):\n",
    "        # MNISTターゲットの場合、特定のIDが数字(タイプ2)、他が背景(タイプ1)を表す\n",
    "        # このデモでは、latticeが既にターゲット状態を表していると仮定し、\n",
    "        # ID=MNIST_TARGET_CELL_TYPE を タイプ2、他の非ゼロIDを タイプ1 とする簡易マッピング。\n",
    "        # 実際には、初期状態やMCMCサンプルに対して動的にマッピングする必要がある。\n",
    "        types = torch.zeros_like(lattice, dtype=torch.long, device=lattice.device)\n",
    "        types[(lattice != 0) & (lattice != self.cfg.MNIST_TARGET_CELL_TYPE)] = (\n",
    "            1  # 背景タイプ\n",
    "        )\n",
    "        types[lattice == self.cfg.MNIST_TARGET_CELL_TYPE] = 2  # 数字タイプ\n",
    "        return types.long()\n",
    "\n",
    "    def forward(self, lattice):\n",
    "        batch_size, H, W = lattice.shape\n",
    "        # 1. 接触エネルギー計算\n",
    "        neighbor_ids = get_neighbors(lattice)\n",
    "        lattice_expanded = lattice.unsqueeze(1).expand(-1, 4, -1, -1)\n",
    "        lattice_types = self.map_ids_to_types(lattice)\n",
    "        neighbor_types = self.map_ids_to_types(neighbor_ids.reshape(-1, H, W)).view(\n",
    "            batch_size, 4, H, W\n",
    "        )\n",
    "        lattice_types_expanded = lattice_types.unsqueeze(1).expand(-1, 4, -1, -1)\n",
    "        contact_J_values = self.J[lattice_types_expanded, neighbor_types]\n",
    "        is_different_id = lattice_expanded != neighbor_ids\n",
    "        contact_energy = (\n",
    "            torch.sum(contact_J_values * is_different_id, dim=(1, 2, 3)) / 2.0\n",
    "        )\n",
    "\n",
    "        # 2. 体積制約エネルギー計算 (ここでは細胞ごとの区別を簡単にする)\n",
    "        #    MNISTターゲット学習では、体積制約の役割は限定的になる可能性がある。\n",
    "        cell_volumes = get_cell_volumes(lattice, self.max_cells)\n",
    "        volume_penalty = torch.zeros(batch_size, device=lattice.device)\n",
    "        for b in range(batch_size):\n",
    "            present_cell_ids = torch.unique(lattice[b])\n",
    "            present_cell_ids = present_cell_ids[present_cell_ids > 0]\n",
    "            if len(present_cell_ids) > 0:\n",
    "                volumes_b = cell_volumes[b, present_cell_ids.long()]\n",
    "                # 平均体積に対するペナルティ（簡易的）\n",
    "                avg_volume = volumes_b.mean() if volumes_b.numel() > 0 else 0\n",
    "                penalty_b = self.lambda_volume * (avg_volume - self.target_volume) ** 2\n",
    "                # penalty_b = self.lambda_volume * (volumes_b - self.target_volume)**2\n",
    "                volume_penalty[b] = penalty_b  # .sum()\n",
    "\n",
    "        total_energy = contact_energy + volume_penalty\n",
    "        return total_energy\n",
    "\n",
    "\n",
    "# --- Neural Hamiltonian コンポーネント (変更なし) ---\n",
    "class NHLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.conv_block(features)\n",
    "\n",
    "\n",
    "class NeuralHamiltonian(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        self.max_cells = config.MAX_CELLS\n",
    "        self.lattice_size = config.LATTICE_SIZE\n",
    "        self.initial_embed = nn.Conv2d(\n",
    "            config.MAX_CELLS, config.NH_EMBED_DIM, kernel_size=1\n",
    "        )\n",
    "        nh_layers = []\n",
    "        current_dim = config.NH_EMBED_DIM\n",
    "        current_size = config.LATTICE_SIZE\n",
    "        for i, (hidden_dim, pool_rate) in enumerate(\n",
    "            zip(config.NH_HIDDEN_DIMS, config.NH_POOL_RATES)\n",
    "        ):\n",
    "            nh_layers.append(NHLayer(current_dim, hidden_dim, config.NH_KERNEL_SIZE))\n",
    "            if pool_rate > 1:\n",
    "                nh_layers.append(nn.MaxPool2d(pool_rate))\n",
    "                current_size //= pool_rate\n",
    "            current_dim = hidden_dim\n",
    "        self.nh_network = nn.Sequential(*nh_layers)\n",
    "        final_conv_output_size = current_dim * current_size * current_size\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(final_conv_output_size, config.NH_MLP_DIM),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(config.NH_MLP_DIM, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, lattice):\n",
    "        batch_size, H, W = lattice.shape\n",
    "        lattice_clamped = torch.clamp(lattice.long(), 0, self.max_cells - 1)\n",
    "        one_hot = (\n",
    "            F.one_hot(lattice_clamped, num_classes=self.max_cells)\n",
    "            .permute(0, 3, 1, 2)\n",
    "            .float()\n",
    "        )\n",
    "        embedded = self.initial_embed(one_hot)\n",
    "        features = self.nh_network(embedded)\n",
    "        # === .reshape() を使用 ===\n",
    "        flat_features = features.reshape(batch_size, -1)\n",
    "        # =======================\n",
    "        energy = self.mlp(flat_features).squeeze(-1)\n",
    "        return energy\n",
    "\n",
    "\n",
    "# --- 統合モデル (Closure) (変更なし) ---\n",
    "class NeuralCPM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        self.analytical_h = AnalyticalHamiltonian(config)\n",
    "        self.neural_h = NeuralHamiltonian(config)\n",
    "        self.w_s = config.CLOSURE_WEIGHT_ANALYTICAL\n",
    "        self.w_nn = config.CLOSURE_WEIGHT_NEURAL\n",
    "\n",
    "    def forward(self, lattice):\n",
    "        e_analytical = self.analytical_h(lattice)\n",
    "        e_neural = self.neural_h(lattice)\n",
    "        total_energy = self.w_s * e_analytical + self.w_nn * e_neural\n",
    "        return total_energy\n",
    "\n",
    "\n",
    "# --- 近似サンプラー (ApproxPCPM - 論文 Algorithm 2) (変更なし) ---\n",
    "def approx_pcpm_sampler(model, initial_states, num_steps, num_parallel_flips, temp):\n",
    "    current_states = initial_states.clone()\n",
    "    batch_size, H, W = current_states.shape\n",
    "    for _ in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            current_energy = model(current_states)\n",
    "        neighbor_ids = get_neighbors(current_states)\n",
    "        boundary_mask, boundary_indices_list = get_boundary_sites(\n",
    "            current_states, neighbor_ids\n",
    "        )\n",
    "        proposed_flips_i = []\n",
    "        proposed_flips_j = []\n",
    "        batch_indices_for_flips = []\n",
    "        for b in range(batch_size):\n",
    "            boundary_idx_b = boundary_indices_list[b]\n",
    "            num_boundary = boundary_idx_b.shape[0]\n",
    "            if num_boundary == 0:\n",
    "                continue\n",
    "            sample_indices = torch.randint(\n",
    "                0, num_boundary, (num_parallel_flips,), device=cfg.DEVICE\n",
    "            )\n",
    "            sites_to_flip = boundary_idx_b[sample_indices]\n",
    "            y_coords, x_coords = sites_to_flip[:, 0], sites_to_flip[:, 1]\n",
    "            neighbors_at_sites = neighbor_ids[b, :, y_coords, x_coords]\n",
    "            current_id_at_sites = current_states[b, y_coords, x_coords]\n",
    "            new_cell_ids = torch.zeros(\n",
    "                num_parallel_flips, dtype=torch.long, device=cfg.DEVICE\n",
    "            )\n",
    "            for p in range(num_parallel_flips):\n",
    "                site_id = current_id_at_sites[p]\n",
    "                neigh_ids = neighbors_at_sites[:, p]\n",
    "                valid_neigh = neigh_ids[neigh_ids != site_id]\n",
    "                if len(valid_neigh) > 0:\n",
    "                    chosen_neigh_idx = torch.randint(0, len(valid_neigh), (1,))\n",
    "                    new_cell_ids[p] = valid_neigh[chosen_neigh_idx]\n",
    "                else:\n",
    "                    new_cell_ids[p] = site_id\n",
    "            proposed_flips_i.append(sites_to_flip)\n",
    "            proposed_flips_j.append(new_cell_ids)\n",
    "            batch_indices_for_flips.extend([b] * num_parallel_flips)\n",
    "        if not proposed_flips_i:\n",
    "            continue\n",
    "        all_sites_i = torch.cat(proposed_flips_i, dim=0)\n",
    "        all_new_ids_j = torch.cat(proposed_flips_j, dim=0)\n",
    "        all_batch_idx = torch.tensor(batch_indices_for_flips, device=cfg.DEVICE)\n",
    "        proposed_states_batch = []\n",
    "        original_energies_batch = []\n",
    "        total_flips_processed = 0\n",
    "        # バッチごとにP個の提案状態を作成するループを修正\n",
    "        idx_offset = 0\n",
    "        for b in range(batch_size):\n",
    "            if b >= len(proposed_flips_i):\n",
    "                continue\n",
    "            num_flips_b = proposed_flips_i[b].shape[0]\n",
    "            if num_flips_b == 0:\n",
    "                continue  # フリップ候補がない場合\n",
    "\n",
    "            sites_i_b = proposed_flips_i[b]\n",
    "            new_ids_j_b = proposed_flips_j[b]\n",
    "\n",
    "            states_b = current_states[b].unsqueeze(0).repeat(num_flips_b, 1, 1)\n",
    "            y_coords, x_coords = sites_i_b[:, 0], sites_i_b[:, 1]\n",
    "            states_b[torch.arange(num_flips_b), y_coords, x_coords] = new_ids_j_b\n",
    "            proposed_states_batch.append(states_b)\n",
    "            original_energies_batch.append(current_energy[b].repeat(num_flips_b))\n",
    "            idx_offset += num_flips_b\n",
    "\n",
    "        if not proposed_states_batch:\n",
    "            continue\n",
    "\n",
    "        all_proposed_states = torch.cat(proposed_states_batch, dim=0)\n",
    "        all_original_energies = torch.cat(original_energies_batch, dim=0)\n",
    "        with torch.no_grad():\n",
    "            all_proposed_energies = model(all_proposed_states)\n",
    "        delta_energies = all_proposed_energies - all_original_energies\n",
    "        accept_prob = torch.exp(-delta_energies / temp)\n",
    "        accept_prob = torch.clamp(accept_prob, max=1.0)\n",
    "        random_uniform = torch.rand_like(accept_prob)\n",
    "        accepted_flips_mask = random_uniform < accept_prob\n",
    "        accepted_batch_idx = all_batch_idx[accepted_flips_mask]\n",
    "        accepted_sites_i = all_sites_i[accepted_flips_mask]\n",
    "        accepted_new_ids_j = all_new_ids_j[accepted_flips_mask]\n",
    "        if accepted_sites_i.shape[0] > 0:\n",
    "            y_coords_acc, x_coords_acc = accepted_sites_i[:, 0], accepted_sites_i[:, 1]\n",
    "            current_states[accepted_batch_idx, y_coords_acc, x_coords_acc] = (\n",
    "                accepted_new_ids_j\n",
    "            )\n",
    "    return current_states.detach()\n",
    "\n",
    "\n",
    "# --- データ生成/取得関数 ---\n",
    "def generate_initial_config(batch_size, lattice_size, num_cells):\n",
    "    \"\"\"\n",
    "    ランダムな初期状態を生成する (変更なし、ただし細胞数はNHの都合)。\n",
    "    \"\"\"\n",
    "    lattices = torch.zeros(\n",
    "        batch_size, lattice_size, lattice_size, dtype=torch.long, device=cfg.DEVICE\n",
    "    )\n",
    "    center = lattice_size // 2\n",
    "    radius = lattice_size // 4\n",
    "    for b in range(batch_size):\n",
    "        # MCMCサンプルは多様な細胞IDを持つ可能性があるため、初期状態も多様にする\n",
    "        temp_cell_ids = list(range(1, num_cells + 1))  # 1からnum_cellsまでのID\n",
    "        random.shuffle(temp_cell_ids)\n",
    "        cell_id_idx = 0\n",
    "        for y in range(lattice_size):\n",
    "            for x in range(lattice_size):\n",
    "                if (y - center) ** 2 + (x - center) ** 2 < radius**2:\n",
    "                    if random.random() < 0.1:\n",
    "                        if cell_id_idx < len(temp_cell_ids):\n",
    "                            lattices[b, y, x] = temp_cell_ids[cell_id_idx]\n",
    "                            cell_id_idx += 1\n",
    "                        # else: keep as medium (0)\n",
    "    return lattices  # タイプ情報は返さない\n",
    "\n",
    "\n",
    "def create_target_lattice_from_mnist(images, threshold, target_cell_type):\n",
    "    \"\"\"\n",
    "    MNIST画像バッチからターゲット格子状態を生成する。\n",
    "    Args:\n",
    "        images (torch.Tensor): MNIST画像のバッチ (B, 1, H, W), 値は [0, 1]\n",
    "        threshold (float): 二値化の閾値\n",
    "        target_cell_type (int): 数字部分に割り当てる細胞タイプID\n",
    "    Returns:\n",
    "        torch.Tensor: ターゲット格子状態 (B, H, W)\n",
    "                      数字部分 = target_cell_type, 背景 = 1 (タイプ1), 培地はなし\n",
    "    \"\"\"\n",
    "    batch_size, _, H, W = images.shape\n",
    "    # 閾値処理で二値化 (数字部分が True になるマスク)\n",
    "    digit_mask = (images > threshold).squeeze(1)  # (B, H, W)\n",
    "\n",
    "    # ターゲット格子を作成\n",
    "    target_lattice = torch.ones(\n",
    "        batch_size, H, W, dtype=torch.long, device=images.device\n",
    "    )  # まず全体を背景(タイプ1)で埋める\n",
    "    target_lattice[digit_mask] = (\n",
    "        target_cell_type  # 数字部分をターゲットタイプIDで上書き\n",
    "    )\n",
    "\n",
    "    return target_lattice\n",
    "\n",
    "\n",
    "def get_training_batch(current_iter, dataloader):\n",
    "    \"\"\"\n",
    "    MNISTデータローダーからバッチを取得し、ターゲット格子を生成する。\n",
    "    データローダーが終端に達したら、再度イテレータを作成する。\n",
    "    \"\"\"\n",
    "    global train_iter  # グローバル変数を更新するため宣言\n",
    "    try:\n",
    "        # データローダーから次のバッチを取得\n",
    "        images, _ = next(current_iter)  # ラベルは使用しない\n",
    "    except StopIteration:\n",
    "        # データローダーが終端に達した場合、再度イテレータを作成\n",
    "        print(\"DataLoader reached end, restarting iterator.\")\n",
    "        train_iter = iter(dataloader)\n",
    "        images, _ = next(train_iter)\n",
    "\n",
    "    images = images.to(cfg.DEVICE)\n",
    "    # MNIST画像からターゲット格子を生成\n",
    "    target_lattice = create_target_lattice_from_mnist(\n",
    "        images, cfg.MNIST_THRESHOLD, cfg.MNIST_TARGET_CELL_TYPE\n",
    "    )\n",
    "    return target_lattice\n",
    "\n",
    "\n",
    "# --- 学習ループ ---\n",
    "def train():\n",
    "    global train_iter  # get_training_batch で更新するためグローバル宣言\n",
    "    if train_iter is None:\n",
    "        print(\"MNIST DataLoader not available. Training cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"使用デバイス: {cfg.DEVICE}\")\n",
    "    model = NeuralCPM(cfg).to(cfg.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "\n",
    "    # MCMCチェーンはランダムな初期状態で初期化\n",
    "    mcmc_chains = generate_initial_config(\n",
    "        cfg.BATCH_SIZE, cfg.LATTICE_SIZE, cfg.MAX_CELLS - 1\n",
    "    ).to(cfg.DEVICE)\n",
    "\n",
    "    losses = []\n",
    "    fig, axes = plt.subplots(\n",
    "        2, cfg.NUM_VIZ_SAMPLES, figsize=(cfg.NUM_VIZ_SAMPLES * 3, 6)\n",
    "    )  # 2行にしてターゲットも表示\n",
    "    plt.ion()\n",
    "\n",
    "    start_time = time.time()\n",
    "    for step in tqdm(range(cfg.TRAINING_STEPS), desc=\"Training Steps\"):\n",
    "        # 1. ポジティブサンプルの取得 (MNISTデータから生成)\n",
    "        x_pos = get_training_batch(train_iter, trainloader)\n",
    "\n",
    "        # 2. ネガティブサンプルの取得 (MCMC)\n",
    "        x_neg = approx_pcpm_sampler(\n",
    "            model,\n",
    "            mcmc_chains,\n",
    "            cfg.MCMC_STEPS_PER_UPDATE,\n",
    "            cfg.MCMC_PARALLEL_FLIPS,\n",
    "            cfg.MCMC_TEMP,\n",
    "        )\n",
    "        mcmc_chains = x_neg\n",
    "\n",
    "        # Optional: チェーンリセット\n",
    "        reset_mask = (\n",
    "            torch.rand(cfg.BATCH_SIZE, device=cfg.DEVICE)\n",
    "            < cfg.PERSISTENT_CHAIN_PROB_RESET\n",
    "        )\n",
    "        num_reset = reset_mask.sum().item()\n",
    "        if num_reset > 0:\n",
    "            new_chains = generate_initial_config(\n",
    "                num_reset, cfg.LATTICE_SIZE, cfg.MAX_CELLS - 1\n",
    "            ).to(cfg.DEVICE)\n",
    "            mcmc_chains[reset_mask] = new_chains\n",
    "\n",
    "        # 3. エネルギー計算\n",
    "        e_pos = model(x_pos)\n",
    "        e_neg = model(x_neg.detach())\n",
    "\n",
    "        # 4. 損失計算\n",
    "        loss_mle = e_pos.mean() - e_neg.mean()\n",
    "        loss_reg = cfg.REGULARIZATION_LAMBDA * (e_pos**2 + e_neg**2).mean()\n",
    "        loss = loss_mle + loss_reg\n",
    "\n",
    "        # 5. Backpropagation and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # --- 可視化 ---\n",
    "        if step % cfg.VISUALIZE_EVERY == 0 or step == cfg.TRAINING_STEPS - 1:\n",
    "            print(\n",
    "                f\"\\nステップ: {step}, 損失: {loss.item():.4f}, E_pos 平均: {e_pos.mean().item():.4f}, E_neg 平均: {e_neg.mean().item():.4f}\"\n",
    "            )\n",
    "\n",
    "            viz_samples_neg = mcmc_chains[: cfg.NUM_VIZ_SAMPLES].cpu().numpy()\n",
    "            viz_samples_pos = (\n",
    "                x_pos[: cfg.NUM_VIZ_SAMPLES].cpu().numpy()\n",
    "            )  # 対応するポジティブサンプルも表示\n",
    "\n",
    "            for i in range(cfg.NUM_VIZ_SAMPLES):\n",
    "                # ポジティブサンプル（ターゲット）の表示 (上段)\n",
    "                ax_pos = axes[0, i] if cfg.NUM_VIZ_SAMPLES > 1 else axes[0]\n",
    "                ax_pos.clear()\n",
    "                cmap_pos = plt.cm.get_cmap(\n",
    "                    \"gray\", 3\n",
    "                )  # ターゲットはシンプルに表示 (0:培地(未使用), 1:背景, 2:数字)\n",
    "                ax_pos.imshow(\n",
    "                    viz_samples_pos[i],\n",
    "                    cmap=cmap_pos,\n",
    "                    vmin=0,\n",
    "                    vmax=cfg.NUM_CELL_TYPES - 1,\n",
    "                    interpolation=\"nearest\",\n",
    "                )\n",
    "                ax_pos.set_title(f\"Target {i}\")\n",
    "                ax_pos.axis(\"off\")\n",
    "\n",
    "                # ネガティブサンプル（MCMC）の表示 (下段)\n",
    "                ax_neg = axes[1, i] if cfg.NUM_VIZ_SAMPLES > 1 else axes[1]\n",
    "                ax_neg.clear()\n",
    "                cmap_mcmc = plt.cm.get_cmap(\"tab20\", cfg.MAX_CELLS)\n",
    "                ax_neg.imshow(\n",
    "                    viz_samples_neg[i],\n",
    "                    cmap=cmap_mcmc,\n",
    "                    vmin=0,\n",
    "                    vmax=cfg.MAX_CELLS - 1,\n",
    "                    interpolation=\"nearest\",\n",
    "                )\n",
    "                ax_neg.set_title(f\"MCMC {i}\")\n",
    "                ax_neg.axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"ターゲット vs MCMCサンプル @ ステップ {step}\")\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.draw()\n",
    "            plt.pause(0.1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n学習終了 ({end_time - start_time:.2f}秒)\")\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"学習損失の推移\")\n",
    "    plt.xlabel(\"学習ステップ\")\n",
    "    plt.ylabel(\"損失\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- メイン実行ブロック ---\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
