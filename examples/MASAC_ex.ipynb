{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281da039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'normalize_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 340\u001b[0m\n\u001b[1;32m    331\u001b[0m env \u001b[38;5;241m=\u001b[39m SimpleMAEnvRevised(n_agents\u001b[38;5;241m=\u001b[39mN_AGENTS, \n\u001b[1;32m    332\u001b[0m                          state_dim_per_agent\u001b[38;5;241m=\u001b[39mSTATE_DIM_PER_AGENT, \n\u001b[1;32m    333\u001b[0m                          act_dim_per_agent\u001b[38;5;241m=\u001b[39mACTION_DIM_PER_AGENT,\n\u001b[1;32m    334\u001b[0m                          episode_len\u001b[38;5;241m=\u001b[39mEPISODE_LEN)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# SACモデルのインスタンス化\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# verbose=1で学習の進捗を表示\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# buffer_sizeを小さくしてメモリ使用量を抑える（デモ用）\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# learning_startsを小さくして早く学習を開始\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMultiAgentSACPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ma_sac_tensorboard/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# リプレイバッファのサイズ\u001b[39;49;00m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# このステップ数から学習を開始\u001b[39;49;00m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# バッチサイズ\u001b[39;49;00m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 1ステップごとに学習\u001b[39;49;00m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 1ステップごとに1回勾配更新\u001b[39;49;00m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 学習率 (定数またはスケジュール)\u001b[39;49;00m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# 割引率\u001b[39;49;00m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# ソフトアップデートの係数\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m学習を開始します...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    357\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTOTAL_TIMESTEPS, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# log_intervalでTensorBoardへのログ記録頻度を指定\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/lib/python3.9/site-packages/stable_baselines3/sac/sac.py:157\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef_optimizer: Optional[th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/lib/python3.9/site-packages/stable_baselines3/sac/sac.py:160\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 190\u001b[0m, in \u001b[0;36mMultiAgentSACPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[1;32m    183\u001b[0m     single_agent_high_act \u001b[38;5;241m=\u001b[39m action_space\u001b[38;5;241m.\u001b[39mhigh\n\u001b[1;32m    185\u001b[0m single_agent_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[1;32m    186\u001b[0m     low\u001b[38;5;241m=\u001b[39msingle_agent_low_act, high\u001b[38;5;241m=\u001b[39msingle_agent_high_act,\n\u001b[1;32m    187\u001b[0m     shape\u001b[38;5;241m=\u001b[39msingle_agent_act_shape, dtype\u001b[38;5;241m=\u001b[39maction_space\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    188\u001b[0m )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_agent_action_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# アクターが直接扱う単一エージェントの行動空間\u001b[39;49;00m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_arch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_std_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_std_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_expln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_expln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_critics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_critics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_features_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_features_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_action_space \u001b[38;5;241m=\u001b[39m action_space\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/lib/python3.9/site-packages/stable_baselines3/sac/policies.py:278\u001b[0m, in \u001b[0;36mSACPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    269\u001b[0m     {\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_critics\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_critics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     }\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor \u001b[38;5;241m=\u001b[39m share_features_extractor\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/lib/python3.9/site-packages/stable_baselines3/sac/policies.py:289\u001b[0m, in \u001b[0;36mSACPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    284\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs,\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# Do not optimize the shared features extractor with the critic loss\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# otherwise, there are gradient computation issues\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     critic_parameters \u001b[38;5;241m=\u001b[39m [param \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_extractor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name]\n",
      "Cell \u001b[0;32mIn[2], line 225\u001b[0m, in \u001b[0;36mMultiAgentSACPolicy.make_critic\u001b[0;34m(self, features_extractor)\u001b[0m\n\u001b[1;32m    221\u001b[0m critic_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# net_arch は critic_kwargs に既に入っているはず (SACPolicy.__init__で設定)\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# action_space は critic_kwargs[\"action_space\"] (single-agent)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# observation_space は critic_kwargs[\"observation_space\"] (multi-agent)\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMultiAgentContinuousCritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcritic_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'normalize_images'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from typing import Optional, Union, Any, Type, Dict, List, Tuple # Tuple を追加\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    FlattenExtractor,\n",
    "    create_mlp, # MultiAgentContinuousCritic で使用\n",
    "    get_actor_critic_arch, # MultiAgentSACPolicy で使用\n",
    ")\n",
    "from stable_baselines3.common.type_aliases import Schedule, PyTorchObs\n",
    "from stable_baselines3.common.utils import get_schedule_fn # 学習率スケジュール用\n",
    "# SACPolicy が内部で使用するコンポーネント\n",
    "from stable_baselines3.sac.policies import SACPolicy, Actor, ContinuousCritic\n",
    "\n",
    "\n",
    "# 2. カスタムクリティック\n",
    "class MultiAgentContinuousCritic(ContinuousCritic):\n",
    "    \"\"\"\n",
    "    マルチエージェント用のContinuousCritic。\n",
    "    リプレイバッファからのactions (B, N, D_act) を (B*N, D_act) に変形して処理する。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space, # Multi-agent observation space\n",
    "        action_space: spaces.Space,      # Single-agent action space\n",
    "        net_arch: List[int],\n",
    "        features_extractor: nn.Module,   # Should be MultiAgentFlattenExtractor instance\n",
    "        features_dim: int,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        n_critics: int = 2,\n",
    "        share_features_extractor: bool = True, # SACPolicy default\n",
    "        n_agents: int = 1, # 追加パラメータ\n",
    "    ):\n",
    "        # ContinuousCriticの__init__を呼び出すために、必要な引数を設定\n",
    "        # features_extractor は BasePolicy._wrap_features_extractor でラップされる前のものを想定\n",
    "        super(ContinuousCritic, self).__init__( # MROを考慮し、ContinuousCriticの親であるBasePolicyの__init__を呼び出す\n",
    "             observation_space=observation_space,\n",
    "             action_space=action_space, # Single-agent action space\n",
    "             features_extractor=features_extractor, # ここで渡されるのは MultiAgentFlattenExtractor のはず\n",
    "             normalize_images=False, # normalize_images は MlpExtractor には直接影響しない\n",
    "        )\n",
    "        # BasePolicyの__init__でfeatures_extractorが設定される\n",
    "        # ContinuousCriticの__init__の残りの部分を手動で実行\n",
    "        self.n_agents = n_agents\n",
    "        self.share_features_extractor = share_features_extractor # 通常True for SAC's critic if actor shares\n",
    "\n",
    "        # ContinuousCritic のネットワーク作成部分を模倣\n",
    "        # アクションの次元は単一エージェントのもの\n",
    "        action_dim = spaces.utils.flatdim(action_space)\n",
    "        \n",
    "        self.q_networks = []\n",
    "        for i in range(n_critics):\n",
    "            # features_dim は MultiAgentFlattenExtractor の出力次元 (単一エージェントのフラット化観測次元)\n",
    "            q_net_input_dim = features_dim + action_dim\n",
    "            q_net = create_mlp(q_net_input_dim, 1, net_arch, activation_fn)\n",
    "            q_net = nn.Sequential(*q_net)\n",
    "            self.add_module(f\"qf{i}\", q_net)\n",
    "            self.q_networks.append(q_net)\n",
    "\n",
    "    def forward(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, ...]:\n",
    "        # obs: (B, N, D_obs) from replay buffer or direct call\n",
    "        # actions: (B, N, D_act) from replay buffer\n",
    "        \n",
    "        # self.extract_features は BasePolicy のメソッドで、self.features_extractor を使う\n",
    "        # self.features_extractor は MultiAgentFlattenExtractor のインスタンス\n",
    "        # extracted_features の形状: (B * N, features_dim)\n",
    "        extracted_features = self.extract_features(obs)\n",
    "\n",
    "        # actions を (B * N, D_act_single_agent) に reshape\n",
    "        batch_size = actions.shape[0]\n",
    "        # self.action_space は単一エージェントの行動空間 (SACPolicyの__init__で設定)\n",
    "        # 単一エージェントの行動次元を取得\n",
    "        single_agent_action_dim = self.action_space.shape[0]\n",
    "        reshaped_actions = actions.reshape(batch_size * self.n_agents, single_agent_action_dim)\n",
    "        \n",
    "        q_values = []\n",
    "        for q_net in self.q_networks:\n",
    "            # q_net の入力は (extracted_features と reshaped_actions の結合)\n",
    "            q_input = th.cat([extracted_features, reshaped_actions], dim=1)\n",
    "            q_values.append(q_net(q_input))\n",
    "        return tuple(q_values)\n",
    "\n",
    "\n",
    "# 3. マルチエージェント SAC ポリシー (make_critic を修正)\n",
    "class CPM_SAC_Policy(SACPolicy):\n",
    "    \"\"\"\n",
    "    マルチエージェント強化学習のための SAC (Soft Actor-Critic) ポリシー。\n",
    "    MultiAgentContinuousCritic を使用する。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Box,\n",
    "        action_space: spaces.Box,\n",
    "        lr_schedule: Schedule,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space, # アクターが直接扱う単一エージェントの行動空間\n",
    "            lr_schedule=lr_schedule,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # make_critic をオーバーライドして MultiAgentContinuousCritic を使用\n",
    "    def make_critic(self, features_extractor: Optional[BaseFeaturesExtractor] = None) -> ContinuousCritic:\n",
    "        # critic_kwargs は SACPolicy の __init__ で準備される\n",
    "        critic_kwargs = self.critic_kwargs.copy() # self.critic_kwargs を使用\n",
    "        if features_extractor is not None:\n",
    "             critic_kwargs[\"features_extractor\"] = features_extractor\n",
    "             # Update features_dim if features_extractor is provided\n",
    "             critic_kwargs[\"features_dim\"] = features_extractor.features_dim\n",
    "\n",
    "\n",
    "        # MultiAgentContinuousCritic に n_agents を渡す\n",
    "        critic_kwargs[\"n_agents\"] = self.n_agents\n",
    "        # net_arch は critic_kwargs に既に入っているはず (SACPolicy.__init__で設定)\n",
    "        # action_space は critic_kwargs[\"action_space\"] (single-agent)\n",
    "        # observation_space は critic_kwargs[\"observation_space\"] (multi-agent)\n",
    "        return MultiAgentContinuousCritic(**critic_kwargs).to(self.device)\n",
    "\n",
    "\n",
    "    def _predict(self, observation: PyTorchObs, deterministic: bool = False) -> th.Tensor:\n",
    "        # (以前のコードからコピー)\n",
    "        actions_flat = SACPolicy._predict(self, observation, deterministic=deterministic) # 基底のSACPolicy._predictを明示的に呼ぶ\n",
    "        \n",
    "        if isinstance(observation, dict):\n",
    "            key = next(iter(observation))\n",
    "            batch_size = observation[key].shape[0]\n",
    "        else:\n",
    "            batch_size = observation.shape[0]\n",
    "        \n",
    "        action_shape_per_agent = self.original_action_space.shape[1:]\n",
    "        actions = actions_flat.reshape(batch_size, self.n_agents, *action_shape_per_agent)\n",
    "        return actions\n",
    "\n",
    "    def _get_constructor_parameters(self) -> dict[str, Any]:\n",
    "        # (以前のコードからコピー)\n",
    "        data = super()._get_constructor_parameters()\n",
    "        data[\"action_space\"] = self.original_action_space\n",
    "        return data\n",
    "\n",
    "# 4. 簡単なマルチエージェント環境\n",
    "class SimpleMAEnvRevised(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, n_agents=2, state_dim_per_agent=2, act_dim_per_agent=1, episode_len=100):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim_per_agent = state_dim_per_agent\n",
    "        self.act_dim_per_agent = act_dim_per_agent\n",
    "        self.episode_len = episode_len\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1,\n",
    "                                       shape=(self.n_agents, self.act_dim_per_agent),\n",
    "                                       dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.n_agents, self.state_dim_per_agent),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.agent_states = np.zeros((self.n_agents, self.state_dim_per_agent), dtype=np.float32)\n",
    "        self.target_states = np.zeros((self.n_agents, self.state_dim_per_agent), dtype=np.float32)\n",
    "        if self.state_dim_per_agent > 0: # ターゲットを固定で設定\n",
    "             self.target_states[:, :self.state_dim_per_agent // 2] = 0.5 # 例: 半分は0.5\n",
    "             self.target_states[:, self.state_dim_per_agent // 2:] = -0.5 # 残り半分は-0.5\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.agent_states.copy()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.agent_states = self.np_random.uniform(low=-1, high=1, size=(self.n_agents, self.state_dim_per_agent)).astype(np.float32)\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        # Action clipping, as policy output might be outside [-1, 1] before squashing\n",
    "        # However, SAC actor uses Tanh, so it's already in [-1, 1]\n",
    "        # action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "\n",
    "        force_applied = np.zeros_like(self.agent_states)\n",
    "        dim_to_apply = min(self.act_dim_per_agent, self.state_dim_per_agent)\n",
    "        force_applied[:, :dim_to_apply] = action[:, :dim_to_apply]\n",
    "        \n",
    "        self.agent_states += force_applied * 0.1\n",
    "        self.agent_states = np.clip(self.agent_states, -5, 5) # 状態が発散しないようにクリップ\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        total_reward = 0.0\n",
    "        for i in range(self.n_agents):\n",
    "            distance = np.linalg.norm(self.agent_states[i] - self.target_states[i])\n",
    "            total_reward -= distance # 距離が小さいほど報酬が高い\n",
    "        \n",
    "        reward = float(total_reward) # 全エージェントの報酬の合計 (または平均)\n",
    "\n",
    "        terminated = self.current_step >= self.episode_len\n",
    "        truncated = False\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if \"human\" in self.metadata[\"render_modes\"]:\n",
    "            for i in range(self.n_agents):\n",
    "                print(f\"Agent {i}: State={np.round(self.agent_states[i],2)}, Target={np.round(self.target_states[i],2)}\")\n",
    "            print(f\"Step: {self.current_step}, Total Reward (current step): {self.reward_for_render:.2f}\") # 最後に計算された報酬を表示\n",
    "            print(\"-\" * 20)\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# 5. メインの実行ブロック\n",
    "if __name__ == '__main__':\n",
    "    N_AGENTS = 3\n",
    "    STATE_DIM_PER_AGENT = 2\n",
    "    ACTION_DIM_PER_AGENT = 1\n",
    "    EPISODE_LEN = 200\n",
    "    TOTAL_TIMESTEPS = 30000 # 学習ステップ数を増やす\n",
    "\n",
    "    # 環境のインスタンス化\n",
    "    env = SimpleMAEnvRevised(n_agents=N_AGENTS, \n",
    "                             state_dim_per_agent=STATE_DIM_PER_AGENT, \n",
    "                             act_dim_per_agent=ACTION_DIM_PER_AGENT,\n",
    "                             episode_len=EPISODE_LEN)\n",
    "\n",
    "    # SACモデルのインスタンス化\n",
    "    # verbose=1で学習の進捗を表示\n",
    "    # buffer_sizeを小さくしてメモリ使用量を抑える（デモ用）\n",
    "    # learning_startsを小さくして早く学習を開始\n",
    "    model = SAC(\n",
    "        MultiAgentSACPolicy,\n",
    "        env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./ma_sac_tensorboard/\",\n",
    "        buffer_size=10000, # リプレイバッファのサイズ\n",
    "        learning_starts=100, # このステップ数から学習を開始\n",
    "        batch_size=64,       # バッチサイズ\n",
    "        train_freq=(1, \"step\"), # 1ステップごとに学習\n",
    "        gradient_steps=1,       # 1ステップごとに1回勾配更新\n",
    "        learning_rate=3e-4,     # 学習率 (定数またはスケジュール)\n",
    "        gamma=0.99,             # 割引率\n",
    "        tau=0.005,              # ソフトアップデートの係数\n",
    "        device=\"auto\"\n",
    "    )\n",
    "\n",
    "    print(\"学習を開始します...\")\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=10) # log_intervalでTensorBoardへのログ記録頻度を指定\n",
    "    print(\"学習が完了しました。\")\n",
    "\n",
    "    # 学習済みモデルの保存\n",
    "    model.save(\"ma_sac_model\")\n",
    "    print(\"モデルを ma_sac_model.zip として保存しました。\")\n",
    "\n",
    "    # 学習済みモデルのロード (オプション)\n",
    "    # del model \n",
    "    # model = SAC.load(\"ma_sac_model\", env=env, policy_class=MultiAgentSACPolicy)\n",
    "    # print(\"モデルをロードしました。\")\n",
    "\n",
    "    # 学習後のポリシーで環境を数ステップ実行してみる\n",
    "    print(\"\\n学習後のエージェントの動作テスト:\")\n",
    "    obs, info = env.reset()\n",
    "    env.reward_for_render = 0 # render用\n",
    "    for i in range(EPISODE_LEN + 5):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.reward_for_render = reward # render用\n",
    "        if i % 20 == 0: # 20ステップごとに描画\n",
    "            env.render()\n",
    "        if terminated or truncated:\n",
    "            print(\"エピソード終了\")\n",
    "            obs, info = env.reset()\n",
    "            env.reward_for_render = 0\n",
    "            if i > EPISODE_LEN: # 1エピソード以上実行したら終了\n",
    "                break\n",
    "    \n",
    "    env.close()\n",
    "    print(\"デモを終了します。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50b898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
